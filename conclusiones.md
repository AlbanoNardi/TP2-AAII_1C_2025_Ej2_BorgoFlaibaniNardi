# Agentes Inteligentes con Q-Learning y Deep Q-Learning en Flappy Bird

## Objetivo

El objetivo de este proyecto es entrenar agentes para resolver videojuegos sencillos utilizando tÃ©cnicas de aprendizaje por refuerzo. En particular, se implementan y entrenan dos tipos de agentes para el juego **Flappy Bird**, utilizando la librerÃ­a `PLE (PyGame Learning Environment)`:

- **Ejercicio A:** Agente basado en **Q-Learning** (Q-Table)
- **Ejercicio B:** Agente basado en **Deep Q-Learning** (Red Neuronal)

---

## Ejercicio A: Agente con Q-Learning

### âœ… ImplementaciÃ³n del Agente

El agente utiliza una Q-table para aprender las mejores acciones a partir de estados discretizados del entorno. Para ello, se realizÃ³ un proceso de **ingenierÃ­a de caracterÃ­sticas** que permite transformar el estado continuo del juego en un espacio de estados discretos manejable con un total de 144 posibles estados discretos, resultantes de combinar las distintas variables categorizadas del entorno.

La discretizaciÃ³n incluye:
- **PosiciÃ³n relativa del jugador respecto al centro del tubo** (`player_zone`):
  - Abajo del centro
  - Zona segura (centro)
  - Arriba del centro
- **Distancia normalizada al tubo mÃ¡s prÃ³ximo** (`dist_zone`):
  - Cerca
  - Medio
  - Lejos
- **Tendencia del terreno**, basada en la diferencia de altura entre tubos consecutivos
- **Velocidad vertical del jugador** en 4 bins normalizados
- **PosiciÃ³n binaria del jugador respecto al centro del tubo**

Esta discretizaciÃ³n permite representar el estado con una tupla de 5 elementos discretos, haciendo posible indexar la Q-table de manera eficiente.

### ğŸ‹ï¸ Entrenamiento del Agente

AdemÃ¡s de las recompensas por pasar tubos (definidas por el entorno), se implementÃ³ una **recompensa positiva por mantenerse vivo**:

```python
current_episode_reward += 0.001  # Recompensa por sobrevivir un paso mÃ¡s
````

Esto contribuyÃ³ a mejorar la estabilidad del entrenamiento, favoreciendo polÃ­ticas que maximizan la duraciÃ³n de vida del agente.

### ğŸ§ª Prueba del Agente Entrenado

A continuaciÃ³n se muestran los resultados de cinco episodios de prueba luego del entrenamiento:

```
Episodio 1:  Recompensa = 86.0
Episodio 2:  Recompensa = 219.0
Episodio 3:  Recompensa = 437.0
Episodio 4:  Recompensa = 144.0
Episodio 5:  Recompensa = 13.0
```

Se observa un desempeÃ±o variable pero con episodios de alto rendimiento, lo que indica que el agente logrÃ³ aprender una polÃ­tica efectiva en muchos casos.

---

## Ejercicio B: Agente con Deep Q-Learning

### ğŸ§  Entrenamiento de la Red Neuronal

Para este agente, se utilizÃ³ una red neuronal entrenada para aproximar la Q-table aprendida previamente por el agente basado en Q-learning. La red fue entrenada usando como dataset las tuplas `(estado_discretizado, acciÃ³n Ã³ptima)` extraÃ­das de la Q-table entrenada, lo que permite al modelo generalizar el comportamiento aprendido sin necesidad de explorar nuevamente el entorno.

La red se guardÃ³ como un modelo `TensorFlow SavedModel` en el archivo `flappy_q_nn_model.keras`.

### ğŸ¤– ImplementaciÃ³n del Agente Neuronal

El agente neuronal, implementado en la clase `NNAgent`, utiliza la misma funciÃ³n de discretizaciÃ³n que el agente de Q-learning, manteniendo asÃ­ la coherencia en la representaciÃ³n del estado. Sin embargo, en lugar de consultar una tabla, el agente pasa el estado discretizado como input a la red neuronal y selecciona la acciÃ³n con mayor valor Q predicho.

Esto le permite tomar decisiones mÃ¡s rÃ¡pidas y adaptarse mejor a estados no vistos exactamente durante el entrenamiento.

### ğŸ§ª Prueba del Agente Neuronal

Resultados de los episodios de prueba del agente con red neuronal:
```
Episodio 1: Recompensa = 411.0
Episodio 2: Recompensa = 188.0
Episodio 3: Recompensa = 33.0
Episodio 4: Recompensa = 1097.0
```

Se observan recompensas significativamente altas en algunos episodios, lo que indica que el modelo neuronal logrÃ³ generalizar correctamente la polÃ­tica aprendida y, en ocasiones, superÃ³ el rendimiento del agente basado en tabla.


---

## EjecuciÃ³n

Para ejecutar y testear un agente, se utiliza el script `test_agent.py`, pasando el tipo de agente mediante el parÃ¡metro `--agent`:

```bash
python test_agent.py --agent q_learning
python test_agent.py --agent dqn
```

## Estructura del Proyecto

```
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ agentes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ dq_agent.py
â”‚   â”œâ”€â”€ manual_agent.py
â”‚   â”œâ”€â”€ nn_agent.py
â”‚   â””â”€â”€ random_agent.py
â”œâ”€â”€ ple/
â”‚   â”œâ”€â”€ games/
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ doomwrapper.py
â”‚   â”‚   â”‚   â””â”€â”€ pygamewrapper.py
â”‚   â”‚   â”œâ”€â”€ doom/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ doom.py
â”‚   â”‚   â”œâ”€â”€ flappybird/
â”‚   â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ monsterkong/
â”‚   â”‚   â”‚   â”œâ”€â”€ LICENSE
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ board.py
â”‚   â”‚   â”‚   â”œâ”€â”€ coin.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fireball.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ladder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ monsterPerson.py
â”‚   â”‚   â”‚   â”œâ”€â”€ onBoard.py
â”‚   â”‚   â”‚   â”œâ”€â”€ person.py
â”‚   â”‚   â”‚   â”œâ”€â”€ player.py
â”‚   â”‚   â”‚   â””â”€â”€ wall.py
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ vec2d.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ catcher.py
â”‚   â”‚   â”œâ”€â”€ pixelcopter.py
â”‚   â”‚   â”œâ”€â”€ pong.py
â”‚   â”‚   â”œâ”€â”€ primitives.py
â”‚   â”‚   â”œâ”€â”€ puckworld.py
â”‚   â”‚   â”œâ”€â”€ raycast.py
â”‚   â”‚   â”œâ”€â”€ raycastmaze.py
â”‚   â”‚   â”œâ”€â”€ snake.py
â”‚   â”‚   â””â”€â”€ waterworld.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ple.py
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ FlappyScores.png
â”‚   â””â”€â”€ NNScores.png
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ conclusiones.md
â”œâ”€â”€ flappy_birds_q_table.pkl
â”œâ”€â”€ flappy_q_nn_model.keras
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ test_agent.py
â”œâ”€â”€ train_q_agent.py
â””â”€â”€ train_q_nn.py
```

---

## Requisitos

Instalar las dependencias necesarias:

```bash
pip install -r requirements.txt
```

---

## Conclusiones

### IngenierÃ­a de CaracterÃ­sticas

Para ambos agentes se aplicÃ³ la misma ingenierÃ­a de caracterÃ­sticas, permitiendo representar el entorno continuo del juego mediante una tupla de 5 valores discretos:

- PosiciÃ³n vertical relativa del jugador respecto al centro del tubo (3 zonas)
- Velocidad vertical discretizada en 4 niveles
- Tendencia de altura entre tubos consecutivos (ascendente o no)
- PosiciÃ³n binaria del jugador (arriba o abajo del centro)
- Distancia normalizada al siguiente tubo (3 zonas)

Esto da lugar a **144 combinaciones posibles de estado**, lo cual permite construir una Q-table de tamaÃ±o razonable y suficientemente expresiva para el agente basado en Q-learning, y tambiÃ©n sirve como entrada de bajo dimensionalidad para la red neuronal.

### ComparaciÃ³n de Resultados

Ambos agentes mostraron un rendimiento competente. El agente de Q-learning aprendiÃ³ una polÃ­tica sÃ³lida con recompensas elevadas en muchos episodios. Sin embargo, el agente con red neuronal logrÃ³ resultados incluso superiores en algunos casos, alcanzando una recompensa de **1097.0** en un episodio, gracias a su capacidad de generalizar mÃ¡s allÃ¡ de los estados vistos en la tabla.

La inclusiÃ³n de una pequeÃ±a recompensa por supervivencia resultÃ³ clave para estabilizar el aprendizaje en ambos modelos.

En conclusiÃ³n, aunque el Q-learning es eficaz para este entorno simple, el uso de redes neuronales permite mayor flexibilidad y escalabilidad para escenarios mÃ¡s complejos.
