# Agentes Inteligentes con Q-Learning y Deep Q-Learning en Flappy Bird

## Objetivo

El objetivo de este proyecto es entrenar agentes para resolver videojuegos sencillos utilizando tÃ©cnicas de aprendizaje por refuerzo. En particular, se implementan y entrenan dos tipos de agentes para el juego **Flappy Bird**, utilizando la librerÃ­a `PLE (PyGame Learning Environment)`:

- **Ejercicio A:** Agente basado en **Q-Learning** (Q-Table)
- **Ejercicio B:** Agente basado en **Deep Q-Learning** (Red Neuronal)

---

## Ejercicio A: Agente con Q-Learning

### âœ… ImplementaciÃ³n del Agente

El agente utiliza una Q-table para aprender las mejores acciones a partir de estados discretizados del entorno. Para ello, se realizÃ³ un proceso de **ingenierÃ­a de caracterÃ­sticas** que permite transformar el estado continuo del juego en un espacio de estados discretos manejable con un total de 144 posibles estados discretos, resultantes de combinar las distintas variables categorizadas del entorno.

La discretizaciÃ³n incluye:
- **PosiciÃ³n relativa del jugador respecto al centro del tubo** (`player_zone`):
  - Abajo del centro
  - Zona segura (centro)
  - Arriba del centro
- **Distancia normalizada al tubo mÃ¡s prÃ³ximo** (`dist_zone`):
  - Cerca
  - Medio
  - Lejos
- **Tendencia del terreno**, basada en la diferencia de altura entre tubos consecutivos
- **Velocidad vertical del jugador** en 4 bins normalizados
- **PosiciÃ³n binaria del jugador respecto al centro del tubo**

Esta discretizaciÃ³n permite representar el estado con una tupla de 5 elementos discretos, haciendo posible indexar la Q-table de manera eficiente.

### ğŸ‹ï¸ Entrenamiento del Agente

AdemÃ¡s de las recompensas por pasar tubos (definidas por el entorno), se implementÃ³ una **recompensa positiva por mantenerse vivo**:

```python
current_episode_reward += 0.001  # Recompensa por sobrevivir un paso mÃ¡s
````

Esto contribuyÃ³ a mejorar la estabilidad del entrenamiento, favoreciendo polÃ­ticas que maximizan la duraciÃ³n de vida del agente.

### ğŸ§ª Prueba del Agente Entrenado

A continuaciÃ³n se muestran los resultados de cinco episodios de prueba luego del entrenamiento:

```
Episodio 1:  Recompensa = 86.0
Episodio 2:  Recompensa = 219.0
Episodio 3:  Recompensa = 437.0
Episodio 4:  Recompensa = 144.0
Episodio 5:  Recompensa = 13.0
```

Se observa un desempeÃ±o variable pero con episodios de alto rendimiento, lo que indica que el agente logrÃ³ aprender una polÃ­tica efectiva en muchos casos.

---

## Ejercicio B: Agente con Deep Q-Learning (âš ï¸ en desarrollo)

### ğŸ”„ Entrenamiento de la Red Neuronal

*Pendiente de implementaciÃ³n.*

### ğŸ¤– ImplementaciÃ³n del Agente Neuronal

*Pendiente de implementaciÃ³n.*

### ğŸ§ª Prueba del Agente Neuronal

*Pendiente de implementaciÃ³n.*

---

## EjecuciÃ³n

Para ejecutar y testear un agente, se utiliza el script `test_agent.py`, pasando el tipo de agente mediante el parÃ¡metro `--agent`:

```bash
python test_agent.py --agent q_learning
python test_agent.py --agent dqn
```

## Estructura del Proyecto

```
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ q_learning_agent.py
â”‚   â””â”€â”€ dqn_agent.py
â”œâ”€â”€ test_agent.py
â”œâ”€â”€ train_agent.py
â”œâ”€â”€ conclusiones.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Requisitos

Instalar las dependencias necesarias:

```bash
pip install -r requirements.txt
```

---

## CrÃ©ditos
