# Agentes Inteligentes con Q-Learning y Deep Q-Learning en Flappy Bird

## Objetivo

El objetivo de este proyecto es entrenar agentes para resolver videojuegos sencillos utilizando t√©cnicas de aprendizaje por refuerzo. En particular, se implementan y entrenan dos tipos de agentes para el juego **Flappy Bird**, utilizando la librer√≠a `PLE (PyGame Learning Environment)`:

- **Ejercicio A:** Agente basado en **Q-Learning** (Q-Table)
- **Ejercicio B:** Agente basado en **Deep Q-Learning** (Red Neuronal)

---

## Ejercicio A: Agente con Q-Learning

### ‚úÖ Implementaci√≥n del Agente

El agente utiliza una Q-table para aprender las mejores acciones a partir de estados discretizados del entorno. Para ello, se realiz√≥ un proceso de **ingenier√≠a de caracter√≠sticas** que permite transformar el estado continuo del juego en un espacio de estados discretos manejable con un total de 144 posibles estados discretos, resultantes de combinar las distintas variables categorizadas del entorno.

La discretizaci√≥n incluye:
- **Posici√≥n relativa del jugador respecto al centro del tubo** (`player_zone`):
  - Abajo del centro
  - Zona segura (centro)
  - Arriba del centro
- **Distancia normalizada al tubo m√°s pr√≥ximo** (`dist_zone`):
  - Cerca
  - Medio
  - Lejos
- **Tendencia del terreno**, basada en la diferencia de altura entre tubos consecutivos
- **Velocidad vertical del jugador** en 4 bins normalizados
- **Posici√≥n binaria del jugador respecto al centro del tubo**

Esta discretizaci√≥n permite representar el estado con una tupla de 5 elementos discretos, haciendo posible indexar la Q-table de manera eficiente.

### üèãÔ∏è Entrenamiento del Agente

Adem√°s de las recompensas por pasar tubos (definidas por el entorno), se implement√≥ una **recompensa positiva por mantenerse vivo**:

```python
current_episode_reward += 0.001  # Recompensa por sobrevivir un paso m√°s
````

Esto contribuy√≥ a mejorar la estabilidad del entrenamiento, favoreciendo pol√≠ticas que maximizan la duraci√≥n de vida del agente.

### üß™ Prueba del Agente Entrenado

A continuaci√≥n se muestran los resultados de cinco episodios de prueba luego del entrenamiento:

```
Episodio 1:  Recompensa = 86.0
Episodio 2:  Recompensa = 219.0
Episodio 3:  Recompensa = 437.0
Episodio 4:  Recompensa = 144.0
Episodio 5:  Recompensa = 13.0
```

Se observa un desempe√±o variable pero con episodios de alto rendimiento, lo que indica que el agente logr√≥ aprender una pol√≠tica efectiva en muchos casos.

---

## Ejercicio B: Agente con Deep Q-Learning

### üß† Entrenamiento de la Red Neuronal

Para este agente, se utiliz√≥ una red neuronal entrenada para aproximar la Q-table aprendida previamente por el agente basado en Q-learning. La red fue entrenada usando como dataset las tuplas `(estado_discretizado, acci√≥n √≥ptima)` extra√≠das de la Q-table entrenada, lo que permite al modelo generalizar el comportamiento aprendido sin necesidad de explorar nuevamente el entorno.

La red se guard√≥ como un modelo `TensorFlow SavedModel` en el archivo `flappy_q_nn_model.keras`.

### ü§ñ Implementaci√≥n del Agente Neuronal

El agente neuronal, implementado en la clase `NNAgent`, utiliza la misma funci√≥n de discretizaci√≥n que el agente de Q-learning, manteniendo as√≠ la coherencia en la representaci√≥n del estado. Sin embargo, en lugar de consultar una tabla, el agente pasa el estado discretizado como input a la red neuronal y selecciona la acci√≥n con mayor valor Q predicho.

Esto le permite tomar decisiones m√°s r√°pidas y adaptarse mejor a estados no vistos exactamente durante el entrenamiento.

### üß™ Prueba del Agente Neuronal

Resultados de los episodios de prueba del agente con red neuronal:
```
Episodio 1: Recompensa = 411.0
Episodio 2: Recompensa = 188.0
Episodio 3: Recompensa = 33.0
Episodio 4: Recompensa = 1097.0
```

Se observan recompensas significativamente altas en algunos episodios, lo que indica que el modelo neuronal logr√≥ generalizar correctamente la pol√≠tica aprendida y, en ocasiones, super√≥ el rendimiento del agente basado en tabla.


---

## Ejecuci√≥n

Para ejecutar y testear un agente, se utiliza el script `test_agent.py`, pasando el tipo de agente mediante el par√°metro `--agent`:

```bash
python test_agent.py --agent q_learning
python test_agent.py --agent dqn
```

## Estructura del Proyecto

```
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ q_learning_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ dqn_agent.py
‚îú‚îÄ‚îÄ test_agent.py
‚îú‚îÄ‚îÄ train_agent.py
‚îú‚îÄ‚îÄ conclusiones.md
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## Requisitos

Instalar las dependencias necesarias:

```bash
pip install -r requirements.txt
```

---

## Conclusiones

### Ingenier√≠a de Caracter√≠sticas

Para ambos agentes se aplic√≥ la misma ingenier√≠a de caracter√≠sticas, permitiendo representar el entorno continuo del juego mediante una tupla de 5 valores discretos:

- Posici√≥n vertical relativa del jugador respecto al centro del tubo (3 zonas)
- Velocidad vertical discretizada en 4 niveles
- Tendencia de altura entre tubos consecutivos (ascendente o no)
- Posici√≥n binaria del jugador (arriba o abajo del centro)
- Distancia normalizada al siguiente tubo (3 zonas)

Esto da lugar a **144 combinaciones posibles de estado**, lo cual permite construir una Q-table de tama√±o razonable y suficientemente expresiva para el agente basado en Q-learning, y tambi√©n sirve como entrada de bajo dimensionalidad para la red neuronal.

### Comparaci√≥n de Resultados

Ambos agentes mostraron un rendimiento competente. El agente de Q-learning aprendi√≥ una pol√≠tica s√≥lida con recompensas elevadas en muchos episodios. Sin embargo, el agente con red neuronal logr√≥ resultados incluso superiores en algunos casos, alcanzando una recompensa de **1097.0** en un episodio, gracias a su capacidad de generalizar m√°s all√° de los estados vistos en la tabla.

La inclusi√≥n de una peque√±a recompensa por supervivencia result√≥ clave para estabilizar el aprendizaje en ambos modelos.

En conclusi√≥n, aunque el Q-learning es eficaz para este entorno simple, el uso de redes neuronales permite mayor flexibilidad y escalabilidad para escenarios m√°s complejos.
